{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 为什么要使用结巴\n",
    "--------------\n",
    "- 在做中文文本分析时，一般需要先进行分词，即将连在一起的文字切分成一个一个的词\n",
    "- 结巴是最简单易用的中文分词工具\n",
    "- 结巴的分词效果一般"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 安装结巴\n",
    "----\n",
    "- 在使用结巴分词前，首先需要先安装结巴\n",
    "- 使用`pip install jieba`安装结巴\n",
    "- 结巴源码： https://github.com/fxsjy/jieba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 尝试初次使用\n",
    "---\n",
    "- 使用`jieba.cut` 最简单的调用\n",
    "- 在调用前，需要先通过`import jieba` 引用jieba包\n",
    "- 参数为待分词的字符串"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "words = jieba.cut(\"实施科教兴国战略，强化现代化建设人才支撑\")\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- cut返回的不是一个直接可以print的内容，而是一个可迭代的generator\n",
    "- 要想取出其中的词，需要使用for in操作获得分词后得到的每一个词语(unicode)，或者使用字符串的join函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "words = jieba.cut(\"实施科教兴国战略，强化现代化建设人才支撑\")\n",
    "#words是一个可迭代的generator，使用for in遍历访问\n",
    "for word in words:\n",
    "    print (word, end=' ')\n",
    "\n",
    "print (\"----------\")\n",
    "\n",
    "words2= jieba.cut(\"实施科教兴国战略，强化现代化建设人才支撑\")\n",
    "#words是一个可迭代的generator，可以使用string.join打印\n",
    "print (\"/\".join(words2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 迭代器中的内容只能访问一次（就像一个水管，里面有水，放完了就没了）\n",
    "- 再次迭代将无法再获取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#只能迭代一次，再次迭代将无法再获取数据\n",
    "for word in words:\n",
    "    print (word, end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 如果需要多次操作，可以将获得的迭代器放到list中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = jieba.cut(\"实施科教兴国战略，强化现代化建设人才支撑\")\n",
    "#words是一个可迭代的generator\n",
    "wordlist = []\n",
    "\n",
    "#将分词结果放在wordlist 列表中存储\n",
    "for word in words:\n",
    "    wordlist.append(word)\n",
    "\n",
    "#列表中存储的词可以多次访问，不会丢失\n",
    "print (\"第一次访问\")\n",
    "for word in wordlist:\n",
    "    print (word, end=' ')\n",
    "\n",
    "print (\"\\n\")\n",
    "print (\"再一次访问\")\n",
    "for word in wordlist:\n",
    "    print (word, end=' ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 好麻烦，能否直接返回一个列表？\n",
    "\n",
    "- 答案是：**可以**！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lcut返回一个list而不是迭代器\n",
    "words = jieba.lcut(\"实施科教兴国战略，强化现代化建设人才支撑\")\n",
    "#words是一个可迭代的generator\n",
    "\n",
    "\n",
    "print (\"第一次访问\")\n",
    "for word in words:\n",
    "    print (word, end=' ')\n",
    "\n",
    "print (\"\\n\")\n",
    "print (\"再一次访问\")\n",
    "for word in words:\n",
    "    print (word, end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# jieba详细用法\n",
    "--------\n",
    "## jieba.cut\n",
    "jieba.cut 方法接受三个输入参数: \n",
    "1. 需要分词的字符串；\n",
    "2. cut_all 参数用来控制是否采用全模式；全模式可能会产生冗余的词\n",
    "3. HMM 参数用来控制是否使用 HMM （隐马尔可夫）模型\n",
    "\n",
    "默认是精确模式\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "seg_list = jieba.cut(\"实施科教兴国战略，强化现代化建设人才支撑\", cut_all=True)\n",
    "print(\"Full Mode: \" + \"/ \".join(seg_list)) # 精确模式\n",
    "\n",
    "seg_list = jieba.cut(\"实施科教兴国战略，强化现代化建设人才支撑\", cut_all=False)\n",
    "print(\"Default Mode: \" + \"/ \".join(seg_list)) # 精确模式\n",
    "\n",
    "\n",
    "seg_list = jieba.cut(\"青年强，则国家强。当代中国青年生逢其时，施展才干的舞台无比广阔，实现梦想的前景无比光明。\", cut_all=True)\n",
    "print(\"Full Mode: \" + \"/ \".join(seg_list)) # 精确模式\n",
    "\n",
    "seg_list = jieba.cut(\"青年强，则国家强。当代中国青年生逢其时，施展才干的舞台无比广阔，实现梦想的前景无比光明。\", cut_all=False)\n",
    "print(\"Default Mode: \" + \"/ \".join(seg_list)) # 精确模式\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## jieba.cut_for_search\n",
    "\n",
    "Cut_for_search 方法接受两个参数：\n",
    "1. 需要分词的字符串；\n",
    "2. 是否使用 HMM 模型。\n",
    "\n",
    "该方法适合用于搜索引擎构建倒排索引的分词，粒度比较细\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "seg_list = jieba.cut_for_search(\"实施科教兴国战略，强化现代化建设人才支撑。青年强，则国家强。当代中国青年生逢其时，施展才干的舞台无比广阔，实现梦想的前景无比光明。\")\n",
    "print(\"cut_for_search: \" + \"/ \".join(seg_list)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## jieba.lcut\n",
    "jieba.lcut 以及 jieba.lcut_for_search 直接返回 list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "seg_list = jieba.lcut(\"我来到中国人民大学\")\n",
    "\n",
    "print(seg_list) \n",
    "\n",
    "seg_list = jieba.lcut_for_search(\"我来到中国人民大学\")\n",
    "print(seg_list) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自定义字典\n",
    "---\n",
    "\n",
    "分词结果中可能会存在不满意的地方，有些词可能没有正确断开"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_list = jieba.lcut(\"当代中国青年生逢其时，施展才干的舞台无比广阔，实现梦想的前景无比光明。\")\n",
    "print(seg_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 开发者可以指定自己自定义的词典，以便包含 jieba 词库里没有的词。\n",
    "-  虽然 jieba 有新词识别能力，但是自行添加新词可以保证更高的正确率\n",
    "-  用法： `jieba.load_userdict(file_name)` # file_name 为文件类对象或自定义词典的路径\n",
    "-  词典格式和 dict.txt 一样，一个词占一行；每一行分三部分：词语、词频（可省略）、词性（可省略），用空格隔开，顺序不可颠倒。file_name 若为路径或二进制方式打开的文件，则文件必须为 UTF-8 编码。\n",
    "- 词频省略时使用自动计算的能保证分出该词的词频。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jieba.load_userdict(\"20_dict.txt\")\n",
    "#生逢其时\n",
    "seg_list = jieba.lcut(\"当代中国青年生逢其时，施展才干的舞台无比广阔，实现梦想的前景无比光明。\")\n",
    "print(seg_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jieba.load_userdict(\"20_dict.txt\")\n",
    "#生逢其时\n",
    "\n",
    "seg_list = jieba.lcut(\"全党全军全国各族人民要紧密团结在党中央周围， \\\n",
    "    牢记空谈误国、实干兴邦，坚定信心、同心同德，埋头苦干、奋勇前进，\\\n",
    "        为全面建设社会主义现代化国家、全面推进中华民族伟大复兴而团结奋斗！\")\n",
    "print(seg_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也可以手动添加词 `jieba.add_word`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jieba.add_word(\"空谈误国\")\n",
    "jieba.add_word(\"实干兴邦\")\n",
    "seg_list = jieba.lcut(\"全党全军全国各族人民要紧密团结在党中央周围， \\\n",
    "    牢记空谈误国、实干兴邦，坚定信心、同心同德，埋头苦干、奋勇前进，\\\n",
    "        为全面建设社会主义现代化国家、全面推进中华民族伟大复兴而团结奋斗！\")\n",
    "\n",
    "print(seg_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 关键词抽取\n",
    "---\n",
    "关键词是文本中能够体现核心内容的词。\n",
    "\n",
    "jieba提供关键词抽取功能，需要首先引入jieba.analyse包\n",
    "\n",
    "`import jieba.analyse`\n",
    "\n",
    "## 通过`extract_tags`调用关键词抽取功能\n",
    "\n",
    "`jieba.analyse.extract_tags(sentence, topK=20, withWeight=False, allowPOS=())`\n",
    "- sentence 为待提取的文本\n",
    "- topK 为返回几个 TF/IDF 权重最大的关键词，默认值为 20\n",
    "- withWeight 为是否一并返回关键词权重值，默认值为 False\n",
    "- allowPOS 仅包括指定词性的词，默认值为空，即不筛选\n",
    "\n",
    "POS的定义请参考:http://playbigdata.ruc.edu.cn/du/poslist\n",
    "\n",
    "\n",
    "> TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的**次数**成正比增加，但同时会随着它在语料库中出现的**频率**成反比下降。TF-IDF加权的各种形式常被搜索引擎应用，作为文件与用户查询之间相关程度的度量或评级。\n",
    "- 词频（term frequency，TF）指的是某一个给定的词语在该文件中出现的频率\n",
    "- 逆向文件频率（inverse document frequency，IDF）是一个词语普遍重要性的度量。某一特定词语的IDF，可以由总文件数目除以包含该词语之文件的数目，再将得到的商取以10为底的对数得到 IDF_i=LOG (|D|/|D_i|)。 |D_i|为包含单词i的文档的数目。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba.analyse\n",
    "\n",
    "keywords = jieba.analyse.extract_tags(\"全党全军全国各族人民要紧密团结在党中央周围， \\\n",
    "    牢记空谈误国、实干兴邦，坚定信心、同心同德，埋头苦干、奋勇前进，\\\n",
    "        为全面建设社会主义现代化国家、全面推进中华民族伟大复兴而团结奋斗！\",5)\n",
    "\n",
    "print (keywords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "尝试一段更长的文本！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = jieba.analyse.extract_tags(\"坚决打赢反腐败斗争攻坚战持久战。腐败是危害党的生命力和战斗力的最大毒瘤， \\\n",
    "    反腐败是最彻底的自我革命。只要存在腐败问题产生的土壤和条件，反腐败斗争就一刻不能停，必须永远吹冲锋号。 \\\n",
    "        坚持不敢腐、不能腐、不想腐一体推进，同时发力、同向发力、综合发力。以零容忍态度反腐惩恶，更加有力遏制增量， \\\n",
    "            更加有效清除存量，坚决查处政治问题和经济问题交织的腐败，坚决防止领导干部成为利益集团和权势团体的代言人、 \\\n",
    "                代理人，坚决治理政商勾连破坏政治生态和经济发展环境问题，决不姑息。深化整治权力集中、资金密集、\\\n",
    "                    资源富集领域的腐败，坚决惩治群众身边的“蝇贪”，严肃查处领导干部配偶、\\\n",
    "                    子女及其配偶等亲属和身边工作人员利用影响力谋私贪腐问题，坚持受贿行贿一起查，\\\n",
    "                    惩治新型腐败和隐性腐败。深化反腐败国际合作，一体构建追逃防逃追赃机制。深化标本兼治，\\\n",
    "                    推进反腐败国家立法，加强新时代廉洁文化建设，教育引导广大党员、干部增强不想腐的自觉，\\\n",
    "                    清清白白做人、干干净净做事，使严厉惩治、规范权力、教育引导紧密结合、协调联动，\\\n",
    "                    不断取得更多制度性成果和更大治理效能。\",5)\n",
    "print (keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果想知道每个词的权重，则使用`withWeight`参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = jieba.analyse.extract_tags(\"坚决打赢反腐败斗争攻坚战持久战。腐败是危害党的生命力和战斗力的最大毒瘤， \\\n",
    "    反腐败是最彻底的自我革命。只要存在腐败问题产生的土壤和条件，反腐败斗争就一刻不能停，必须永远吹冲锋号。 \\\n",
    "        坚持不敢腐、不能腐、不想腐一体推进，同时发力、同向发力、综合发力。以零容忍态度反腐惩恶，更加有力遏制增量， \\\n",
    "            更加有效清除存量，坚决查处政治问题和经济问题交织的腐败，坚决防止领导干部成为利益集团和权势团体的代言人、 \\\n",
    "                代理人，坚决治理政商勾连破坏政治生态和经济发展环境问题，决不姑息。深化整治权力集中、资金密集、\\\n",
    "                    资源富集领域的腐败，坚决惩治群众身边的“蝇贪”，严肃查处领导干部配偶、\\\n",
    "                    子女及其配偶等亲属和身边工作人员利用影响力谋私贪腐问题，坚持受贿行贿一起查，\\\n",
    "                    惩治新型腐败和隐性腐败。深化反腐败国际合作，一体构建追逃防逃追赃机制。深化标本兼治，\\\n",
    "                    推进反腐败国家立法，加强新时代廉洁文化建设，教育引导广大党员、干部增强不想腐的自觉，\\\n",
    "                    清清白白做人、干干净净做事，使严厉惩治、规范权力、教育引导紧密结合、协调联动，\\\n",
    "                    不断取得更多制度性成果和更大治理效能。\",5, withWeight=True)\n",
    "\n",
    "print (keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果只想要其中的名词或者动词，则可以使用allowPOS参数\n",
    "\n",
    "POS的定义请参考:http://playbigdata.ruc.edu.cn/du/poslist\n",
    "\n",
    "a 形容词\n",
    "ad 副形词\n",
    "ag形语素\n",
    "an副形词\n",
    "b区别词\n",
    "c连词\n",
    "d副词\n",
    "df\n",
    "dg副语素\n",
    "e叹词\n",
    "f方位词\n",
    "g语素\n",
    "h前接成分\n",
    "i成语\n",
    "j简称略语\n",
    "k后接成分\n",
    "l习用语\n",
    "m数词\n",
    "mg数语素\n",
    "mq 数量词\n",
    "n名词\n",
    "ng名词性语素\n",
    "nr人名\n",
    "nrfg\n",
    "nrt\n",
    "ns地名\n",
    "nt机构团体\n",
    "nz其他专名\n",
    "o拟声词\n",
    "p介词\n",
    "q量词\n",
    "r代词\n",
    "rg 代语素\n",
    "rr人称代词\n",
    "rz指示代词\n",
    "s处所词\n",
    "t时间词\n",
    "tg时语素\n",
    "u助词\n",
    "ud\n",
    "ug\n",
    "uj\n",
    "ul\n",
    "uv\n",
    "uz\n",
    "v动词\n",
    "vd副动词\n",
    "vg动语素\n",
    "vi不及物动词（内动词）\n",
    "vn名动词\n",
    "vq\n",
    "x非语素字\n",
    "y语气语素\n",
    "z状态词\n",
    "zg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = jieba.analyse.extract_tags(\"坚决打赢反腐败斗争攻坚战持久战。腐败是危害党的生命力和战斗力的最大毒瘤， \\\n",
    "    反腐败是最彻底的自我革命。只要存在腐败问题产生的土壤和条件，反腐败斗争就一刻不能停，必须永远吹冲锋号。 \\\n",
    "        坚持不敢腐、不能腐、不想腐一体推进，同时发力、同向发力、综合发力。以零容忍态度反腐惩恶，更加有力遏制增量， \\\n",
    "            更加有效清除存量，坚决查处政治问题和经济问题交织的腐败，坚决防止领导干部成为利益集团和权势团体的代言人、 \\\n",
    "                代理人，坚决治理政商勾连破坏政治生态和经济发展环境问题，决不姑息。深化整治权力集中、资金密集、\\\n",
    "                    资源富集领域的腐败，坚决惩治群众身边的“蝇贪”，严肃查处领导干部配偶、\\\n",
    "                    子女及其配偶等亲属和身边工作人员利用影响力谋私贪腐问题，坚持受贿行贿一起查，\\\n",
    "                    惩治新型腐败和隐性腐败。深化反腐败国际合作，一体构建追逃防逃追赃机制。深化标本兼治，\\\n",
    "                    推进反腐败国家立法，加强新时代廉洁文化建设，教育引导广大党员、干部增强不想腐的自觉，\\\n",
    "                    清清白白做人、干干净净做事，使严厉惩治、规范权力、教育引导紧密结合、协调联动，\\\n",
    "                    不断取得更多制度性成果和更大治理效能。\",\n",
    "    5, withWeight=True, allowPOS=(\"n\",\"v\")) #\"ns\",\"n\",\n",
    "\n",
    "print (keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于 TextRank 算法的关键词抽取\n",
    "\n",
    "`jieba.analyse.textrank(sentence, topK=20, withWeight=False, allowPOS=('ns', 'n', 'vn', 'v')) `\n",
    "\n",
    "接口相同，注意默认过滤词性。\n",
    "\n",
    "基本思想:\n",
    "- 将待抽取关键词的文本进行分词\n",
    "- 以固定窗口大小(默认为5，通过span属性调整)，词之间的共现关系，构建图\n",
    "- 计算图中节点的PageRank，注意是无向带权图\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = jieba.analyse.textrank(\"坚决打赢反腐败斗争攻坚战持久战。腐败是危害党的生命力和战斗力的最大毒瘤， \\\n",
    "    反腐败是最彻底的自我革命。只要存在腐败问题产生的土壤和条件，反腐败斗争就一刻不能停，必须永远吹冲锋号。 \\\n",
    "        坚持不敢腐、不能腐、不想腐一体推进，同时发力、同向发力、综合发力。以零容忍态度反腐惩恶，更加有力遏制增量， \\\n",
    "            更加有效清除存量，坚决查处政治问题和经济问题交织的腐败，坚决防止领导干部成为利益集团和权势团体的代言人、 \\\n",
    "                代理人，坚决治理政商勾连破坏政治生态和经济发展环境问题，决不姑息。深化整治权力集中、资金密集、\\\n",
    "                    资源富集领域的腐败，坚决惩治群众身边的“蝇贪”，严肃查处领导干部配偶、\\\n",
    "                    子女及其配偶等亲属和身边工作人员利用影响力谋私贪腐问题，坚持受贿行贿一起查，\\\n",
    "                    惩治新型腐败和隐性腐败。深化反腐败国际合作，一体构建追逃防逃追赃机制。深化标本兼治，\\\n",
    "                    推进反腐败国家立法，加强新时代廉洁文化建设，教育引导广大党员、干部增强不想腐的自觉，\\\n",
    "                    清清白白做人、干干净净做事，使严厉惩治、规范权力、教育引导紧密结合、协调联动，\\\n",
    "                    不断取得更多制度性成果和更大治理效能。\",\n",
    "    5, withWeight=True, allowPOS=(\"n\",\"v\"))\n",
    "\n",
    "print (keywords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 词性标注\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`jieba.posseg.POSTokenizer(tokenizer=None)` 新建自定义分词器，tokenizer 参数可指定内部使用的 `jieba.Tokenizer`分词器。`jieba.posseg.dt` 为默认词性标注分词器。\n",
    "\n",
    "标注句子分词后每个词的词性，采用和 ictclas 兼容的标记法\n",
    "a 形容词\n",
    "ad 副形词\n",
    "ag形语素\n",
    "an副形词\n",
    "b区别词\n",
    "c连词\n",
    "d副词\n",
    "df\n",
    "dg副语素\n",
    "e叹词\n",
    "f方位词\n",
    "g语素\n",
    "h前接成分\n",
    "i成语\n",
    "j简称略语\n",
    "k后接成分\n",
    "l习用语\n",
    "m数词\n",
    "mg数语素\n",
    "mq 数量词\n",
    "n名词\n",
    "ng名词性语素\n",
    "nr人名\n",
    "nrfg\n",
    "nrt\n",
    "ns地名\n",
    "nt机构团体\n",
    "nz其他专名\n",
    "o拟声词\n",
    "p介词\n",
    "q量词\n",
    "r代词\n",
    "rg 代语素\n",
    "rr人称代词\n",
    "rz指示代词\n",
    "s处所词\n",
    "t时间词\n",
    "tg时语素\n",
    "u助词\n",
    "ud\n",
    "ug\n",
    "uj\n",
    "ul\n",
    "uv\n",
    "uz\n",
    "v动词\n",
    "vd副动词\n",
    "vg动语素\n",
    "vi不及物动词（内动词）\n",
    "vn名动词\n",
    "vq\n",
    "x非语素字\n",
    "y语气语素\n",
    "z状态词\n",
    "zg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "#通过这一行引入新的分词器！！\n",
    "import jieba.posseg as pseg\n",
    "\n",
    "#使用pseg而不是jieba\n",
    "words = pseg.lcut (\"实施科教兴国战略，强化现代化建设人才支撑\")\n",
    "print (words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "遇到不正确的分词和词性识别结果怎么办？例如下面例子中的“健康宝”属于新词，无法分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "\n",
    "words = pseg.lcut (\"11月1日，“桂东铁路”“桂北铁路”发布通告称，坐火车不再查验核酸证明（进京除外）。\\\n",
    "    通告表示，除进京旅客须出示持北京健康宝绿码和48小时内核酸检测阴性证明方可进站乘车外，\\\n",
    "        其他旅客需出示健康码、行程码测温进站。\")\n",
    "print (words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "个性化词典中设置词性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "\n",
    "jieba.load_userdict(\"temp_dict.txt\")\n",
    "#健康宝 n\n",
    "\n",
    "\n",
    "words = pseg.lcut (\"11月1日，“桂东铁路”“桂北铁路”发布通告称，坐火车不再查验核酸证明（进京除外）。\\\n",
    "    通告表示，除进京旅客须出示持北京健康宝绿码和48小时内核酸检测阴性证明方可进站乘车外，\\\n",
    "        其他旅客需出示健康码、行程码测温进站。\")\n",
    "print (words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 识别文章中的命名实体\n",
    "\n",
    "常见命名实体类型：\n",
    "- n名词\n",
    "- ng名词性语素\n",
    "- nr人名\n",
    "- nrfg\n",
    "- nrt\n",
    "- ns地名\n",
    "- nt机构团体\n",
    "- nz其他专名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "\n",
    "jieba.load_userdict(\"temp_dict.txt\")\n",
    "#健康宝 n\n",
    "\n",
    "\n",
    "words = pseg.lcut (\"澳门特区行政长官贺一诚1日在政府总部向中成药审评专家顾问委员会主席钟南山院士以及各成员颁发聘书。\\\n",
    "    贺一诚表示，委员会在中成药专业审评决策中发挥着重要作用，\\\n",
    "    是开展中成药注册工作、奠定澳门中药产业稳步发展的基石。\\\n",
    "    相信在钟南山院士带领下，委员会将为澳门中成药注册审评以及中成药质量、效用及安全性评估提供专业意见，\\\n",
    "    推动澳门中医药产业高质量发展。\")\n",
    "\n",
    "#遍历分词结果\n",
    "for w,t in words:\n",
    "    if t==\"nr\":\n",
    "        print (w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 应用：词频统计"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词频统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "\n",
    "#加载个性化词典\n",
    "jieba.load_userdict(\"ruc_dict_pos_adv.txt\")\n",
    "\n",
    "#带词性分词\n",
    "words = pseg.lcut (\"靳诺表示完全拥护、坚决服从中央的决定。她强调，\\\n",
    "    中央的决定充分体现了以习近平同志为核心的党中央对中国人民大学的重视和厚望。\\\n",
    "    她回顾了自己与教育和大学结下的不解之缘，尤其是在中国人民大学工作期间的难忘经历和深切感悟。\\\n",
    "    她表示，是组织的培养和信任、大家的帮助和支持，让她有机会为教育这项崇高而又温暖的事业奉献热忱，\\\n",
    "    为大学这个神圣而又温馨的家园倾注心血。在8年半的奋斗时光里，她与全校师生一起，\\\n",
    "    始终以习近平新时代中国特色社会主义思想为指引，深入学习贯彻习近平总书记关于教育的\\\n",
    "    重要论述和致中国人民大学建校80周年贺信精神，紧紧围绕立德树人根本任务，\\\n",
    "    与班子同志们勠力同心、并肩战斗，努力推进学校“双一流”建设，努力加强教师队伍建设，\\\n",
    "    全力推动学校事业高质量发展。今后，她会以一名普通党员、\\\n",
    "    以一名人大人的身份关注学校的发展。现在，走过84载光辉岁月的人民大学\\\n",
    "    已经站在一个新的历史起点，这所具有光荣历史和优良传统的大学，\\\n",
    "    一定能够在新时代为我国高等教育事业繁荣发展，为实现“两个一百年”\\\n",
    "    奋斗目标、实现中华民族伟大复兴的中国梦作出新的更大贡献。\")\n",
    "\n",
    "#词频表\n",
    "word_freq={}\n",
    "\n",
    "#遍历分词结果，使用字典累计词频\n",
    "for w,flag in words:\n",
    "    if w in word_freq:\n",
    "        word_freq[w]+=1\n",
    "    else:\n",
    "        word_freq[w]=1\n",
    "\n",
    "#排序后输出\n",
    "\n",
    "#字典不支持排序，保存到列表中\n",
    "word_list = []\n",
    "for w in word_freq.items():\n",
    "    word_list.append((w[1],w[0]))\n",
    "\n",
    "#列表排序\n",
    "word_list.sort(reverse=True)\n",
    "\n",
    "#输出\n",
    "for freq, word in word_list:\n",
    "    print (word+\"\\t\"+str(freq))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "过滤掉无意义的词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = {\" \", \"的\",\"，\",\"和\",\"。\",\"、\"}\n",
    "for freq, word in word_list:\n",
    "    if word not in stopwords:\n",
    "        print (word+\"\\t\"+str(freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 命名实体词频统计"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "统计其中的实体：人名、地名、机构名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "\n",
    "#加载个性化词典\n",
    "jieba.load_userdict(\"ruc_dict_pos_adv.txt\")\n",
    "\n",
    "#带词性分词\n",
    "words = pseg.lcut (\"靳诺表示完全拥护、坚决服从中央的决定。她强调，\\\n",
    "    中央的决定充分体现了以习近平同志为核心的党中央对中国人民大学的重视和厚望。\\\n",
    "    她回顾了自己与教育和大学结下的不解之缘，尤其是在中国人民大学工作期间的难忘经历和深切感悟。\\\n",
    "    她表示，是组织的培养和信任、大家的帮助和支持，让她有机会为教育这项崇高而又温暖的事业奉献热忱，\\\n",
    "    为大学这个神圣而又温馨的家园倾注心血。在8年半的奋斗时光里，她与全校师生一起，\\\n",
    "    始终以习近平新时代中国特色社会主义思想为指引，深入学习贯彻习近平总书记关于教育的\\\n",
    "    重要论述和致中国人民大学建校80周年贺信精神，紧紧围绕立德树人根本任务，\\\n",
    "    与班子同志们勠力同心、并肩战斗，努力推进学校“双一流”建设，努力加强教师队伍建设，\\\n",
    "    全力推动学校事业高质量发展。今后，她会以一名普通党员、\\\n",
    "    以一名人大人的身份关注学校的发展。现在，走过84载光辉岁月的人民大学\\\n",
    "    已经站在一个新的历史起点，这所具有光荣历史和优良传统的大学，\\\n",
    "    一定能够在新时代为我国高等教育事业繁荣发展，为实现“两个一百年”\\\n",
    "    奋斗目标、实现中华民族伟大复兴的中国梦作出新的更大贡献。\")\n",
    "\n",
    "#词频表\n",
    "word_freq={}\n",
    "\n",
    "#遍历分词结果，使用字典累计词频\n",
    "for w,flag in words:\n",
    "    ###########\n",
    "    #这里加上过滤条件\n",
    "    if flag==\"nr\":\n",
    "        if w in word_freq:\n",
    "            word_freq[w]+=1\n",
    "        else:\n",
    "            word_freq[w]=1\n",
    "\n",
    "#排序后输出\n",
    "\n",
    "#字典不支持排序，保存到列表中\n",
    "stopwords = {\" \", \"的\",\"，\",\"和\",\"。\",\"、\"}\n",
    "word_list = []\n",
    "for w in word_freq.items():\n",
    "    if w[0] not in stopwords:\n",
    "        word_list.append((w[1],w[0]))\n",
    "\n",
    "#列表排序\n",
    "word_list.sort(reverse=True)\n",
    "\n",
    "#输出\n",
    "for freq, word in word_list:    \n",
    "    print (word+\"\\t\"+str(freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于文件的文本分析、词频统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding=utf-8\n",
    "import jieba\n",
    "file = \"20.txt\"\n",
    "content = open(file,\"rb\").read()\n",
    "\n",
    "seg_list = jieba.lcut(content)\n",
    "\n",
    "words = \" \".join(seg_list)\n",
    "outfile = open(\"20_cut.txt\",\"wb\")\n",
    "outfile.write(words.encode('utf-8'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "统计文件中的词频"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "file = \"20.txt\"\n",
    "content = open(file,\"rb\").read()\n",
    "\n",
    "words = jieba.lcut(content)\n",
    "word_freq={}\n",
    "\n",
    "for w in words:\n",
    "    if w in word_freq:\n",
    "        word_freq[w]+=1\n",
    "    else:\n",
    "        word_freq[w]=1\n",
    "\n",
    "word_list = []\n",
    "stopwords = {\" \", \"的\",\"，\",\"和\",\"。\",\"、\",\"\\r\\n\",\"是\"}\n",
    "for w in word_freq.items():\n",
    "    if w[0] not in stopwords:\n",
    "        word_list.append((w[1],w[0]))\n",
    "\n",
    "word_list.sort(reverse=True)\n",
    "\n",
    "for i in range(10):\n",
    "    print(word_list[i])\n",
    "\n",
    "\n",
    "#处理停用词？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#带词性分词\n",
    "words = pseg.lcut (content)\n",
    "\n",
    "#词频表\n",
    "word_freq={}\n",
    "\n",
    "#遍历分词结果，使用字典累计词频\n",
    "for w,flag in words:\n",
    "    ###########\n",
    "    #这里加上过滤条件\n",
    "    if flag==\"nt\":#机构名\n",
    "        if w in word_freq:\n",
    "            word_freq[w]+=1\n",
    "        else:\n",
    "            word_freq[w]=1\n",
    "\n",
    "#排序后输出\n",
    "\n",
    "#字典不支持排序，保存到列表中\n",
    "word_list = []\n",
    "stopwords = {\" \", \"的\",\"，\",\"和\",\"。\",\"、\",\"\\r\\n\",\"是\"}\n",
    "for w in word_freq.items():\n",
    "    if word[0] not in stopwords:\n",
    "        word_list.append((w[1],w[0]))\n",
    "\n",
    "#列表排序\n",
    "word_list.sort(reverse=True)\n",
    "#输出\n",
    "\n",
    "for i in range(10):\n",
    "        freq, word = word_list[i]  \n",
    "        print (word+\"\\t\"+str(freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 课后上机作业\n",
    "-----\n",
    "对homework_input_ruc_news.txt（包含约130篇人大新闻）进行简单的文本分析\n",
    "\n",
    "- 文件格式：每一行一篇新闻，由标题、时间和正文构成\n",
    "- 依次逐行读入该文本中的每一条新闻并对新闻进行：\n",
    "- 分词，输出到新文件中，仍然是每一行一条新闻，词之间用空格隔开；\n",
    "- 抽取关键词，输出到新文件中，仍然是每一行一条新闻，关键词之间用空格隔开；\n",
    "- 统计这些新闻中：\n",
    "- 整体出现最多的前50个词\n",
    "- 出现最多的50个关键词、人名、地点名和机构名，务必使用自定义字典\n",
    "- 撰写一个简单的分析报告，要求格式规范。\n",
    "- 选作：将出现最多的50个词、关键词、人名、地点名和机构名用柱状图或者词云图进行展示，方式和手段不限。\n",
    "\n",
    "思考和拓展：\n",
    "- 更好的分词质量？其他工具？\n",
    "- 提前预习`scrapy`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
